# Projeto de Engenharia de Dados: An√°lise do Dataset Olist

Este projeto implementa um pipeline de dados completo para processar e analisar o dataset de e-commerce da Olist. Utilizando a **Arquitetura Medalh√£o (Bronze, Silver e Gold)**, o pipeline transforma dados brutos em insights de neg√≥cio acion√°veis, garantindo qualidade e rastreabilidade em cada etapa.

## üõ†Ô∏è Stack Tecnol√≥gica

* **Linguagem:** Python 3.10
* **Bibliotecas:** Pandas, Numpy, SQLAlchemy, Psycopg2
* **Banco de Dados:** PostgreSQL 13
* **Ambiente e Orquestra√ß√£o:** Docker e Docker Compose
* **Desenvolvimento:** Jupyter Notebook

---

## üìã Pr√©-requisitos

Antes de iniciar, certifique-se de que os seguintes softwares est√£o instalados e configurados em seu sistema:

1.  **Git:** Essencial para clonar o reposit√≥rio.
2.  **Docker e Docker Compose:** Para executar o ambiente do banco de dados de forma isolada e consistente.
    * **Link de Instala√ß√£o:** [Instale o Docker Desktop](https://www.docker.com/products/docker-desktop/)
3.  **Python 3.10:** A vers√£o da linguagem utilizada no projeto.
    * Verifique sua vers√£o com o comando: `python --version`

---

## üöÄ Guia de Instala√ß√£o e Execu√ß√£o

Siga os passos detalhados abaixo para configurar e executar o projeto em sua m√°quina local.

### 1. Clonar o Reposit√≥rio

Primeiro, clone o reposit√≥rio do projeto para o seu ambiente de trabalho.

```bash
git clone [https://github.com/seu-usuario/seu-repositorio.git](https://github.com/seu-usuario/seu-repositorio.git)
cd seu-repositorio
```

### 2. Configurar o Banco de Dados (PostgreSQL com Docker)

O banco de dados PostgreSQL ser√° executado em um cont√™iner Docker gerenciado pelo Docker Compose.

**a. Altere as credenciais do banco:**

Por seguran√ßa, √© crucial que voc√™ **altere as credenciais padr√£o**. Abra o arquivo `docker-compose.yml` e modifique as vari√°veis de ambiente:

```yaml
version: '3.8'

services:
  db:
    image: postgres:13
    container_name: postgres_db
    restart: always
    environment:
      POSTGRES_USER: altere_para_seu_usuario      # <-- ALTERE AQUI
      POSTGRES_PASSWORD: altere_para_sua_senha    # <-- ALTERE AQUI
      POSTGRES_DB: olist_db                       # <-- (Opcional) Altere o nome do banco
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

**b. Inicie o cont√™iner:**

Com o Docker em execu√ß√£o, navegue at√© a raiz do projeto e execute o comando:

```bash
docker-compose up -d
```
O par√¢metro `-d` executa o cont√™iner em segundo plano. Para verificar se ele est√° ativo, use `docker ps`.

### 3. Configurar o Ambiente Python

**a. Crie e ative uma Virtual Environment (`venv`):**

Isso mant√©m as depend√™ncias do projeto isoladas do restante do sistema.

```bash
# Crie a venv
python -m venv venv

# Ative a venv
# No Windows:
venv\Scripts\activate
# No macOS/Linux:
source venv/bin/activate
```

**b. Instale as depend√™ncias:**

Crie um arquivo `requirements.txt` na raiz do projeto com o seguinte conte√∫do:

```txt
pandas
numpy
sqlalchemy
psycopg2-binary
jupyter
```
Em seguida, instale todas as bibliotecas de uma vez:
```bash
pip install -r requirements.txt
```

### 4. Baixar o Dataset da Olist

**a.** Fa√ßa o download do dataset no site do Kaggle:
* [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

**b.** Descompacte o arquivo `.zip` e mova todos os arquivos `.csv` para dentro da pasta `data/raw` na raiz do projeto. (Crie esta estrutura de pastas se ela n√£o existir).

### 5. Configurar a Conex√£o nos Scripts Python

Seus scripts precisam saber como se conectar ao banco de dados. Abra cada notebook ou script Python e localize a string de conex√£o da **SQLAlchemy**. Atualize-a com **as mesmas credenciais que voc√™ definiu no arquivo `docker-compose.yml`**.

```python
from sqlalchemy import create_engine

# ATEN√á√ÉO: Atualize esta linha com suas credenciais!
# Formato: "postgresql://USUARIO:SENHA@HOST:PORTA/NOME_DO_BANCO"
DATABASE_URL = "postgresql://altere_para_seu_usuario:altere_para_sua_senha@localhost:5432/olist_db"

engine = create_engine(DATABASE_URL)
```

### 6. Executar o Pipeline de Dados

Com o ambiente configurado, execute o pipeline para processar os dados.

**a. Inicie o Jupyter Notebook:**
```bash
jupyter notebook
```

**b. Execute os Notebooks na ordem correta:**

No seu navegador, abra e execute as c√©lulas dos notebooks na sequ√™ncia da Arquitetura Medalh√£o:

1.  **`1_bronze_ingestion.ipynb`**: Carrega os dados brutos dos arquivos CSV para o schema `bronze` no PostgreSQL.
2.  **`2_silver_processing.ipynb`**: L√™ os dados da camada Bronze, aplica limpezas, transforma√ß√µes e salva as tabelas tratadas no schema `silver`.
3.  **`3_gold_aggregation.ipynb`**: Utiliza os dados da camada Silver para criar tabelas de neg√≥cio agregadas no schema `gold`, prontas para an√°lise.

### 7. Verifica√ß√£o dos Resultados

Ap√≥s a execu√ß√£o, voc√™ pode usar uma ferramenta de cliente SQL (DBeaver, pgAdmin, etc.) para se conectar ao banco de dados na porta `5432` e verificar se os schemas (`bronze`, `silver`, `gold`) e suas respectivas tabelas foram criados com sucesso.

### 8. Parando o Ambiente

Quando terminar, pare e remova o cont√™iner do Docker com o seguinte comando. Seus dados ser√£o preservados no volume `postgres_data`.

```bash
docker-compose down
```